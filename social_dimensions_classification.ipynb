{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ekphrasis\n",
    "# !pip install xgboost\n",
    "# !pip install textblob\n",
    "# !pip install keras\n",
    "# !pip install tensorflow\n",
    "# !pip install gensim==3.8.3\n",
    "# !pip install scikit-multilearn\n",
    "# !pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import xgboost, textblob, string, ekphrasis, nltk, re\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "from sklearn.svm import NuSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "sp = SpellCorrector(corpus=\"english\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, ret_preds = False, \n",
    "                ret_probas = False, ret_all = False, is_neural_net=False, probs = True, cross_val = False, valid = True):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if valid:\n",
    "        if cross_val:\n",
    "            f1 = cross_val_scores(classifier, feature_vector_train)\n",
    "        else:\n",
    "            f1 = metrics.f1_score(valid_y[col], predictions)\n",
    "    \n",
    "    if probs:\n",
    "        probas = [i[1] for i in classifier.predict_proba(feature_vector_valid)]\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    if ret_preds:\n",
    "        return predictions\n",
    "    elif ret_probas:\n",
    "        return probas\n",
    "    elif ret_all:\n",
    "        return (f1, metrics.confusion_matrix(valid_y[col], predictions), predictions, probas)\n",
    "    else:\n",
    "        return (f1, metrics.confusion_matrix(valid_y[col], predictions))\n",
    "\n",
    "def get_class(c, cutoff):\n",
    "    return [1 if i>cutoff else 0 for i in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_scores(model, x, verbose=False):\n",
    "    scores = cross_validate(model, x, train_y[col], cv=10, scoring=('f1', 'accuracy'), return_train_score=True)\n",
    "    if verbose:\n",
    "        print('------------------------------------')\n",
    "        print(type(model).__name__)\n",
    "        print('Test f1: {}'.format(np.mean(scores['test_f1'])))\n",
    "        print('Train f1: {}'.format(np.mean(scores['train_f1'])))\n",
    "        print('Test accuracy: {}'.format(np.mean(scores['test_accuracy'])))\n",
    "        print('Train accuracy: {}'.format(np.mean(scores['train_accuracy'])))\n",
    "    \n",
    "    return np.mean(scores['test_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('Data/annotated_tweets_data.csv', index_col=0)\n",
    "tweets_to_train = pd.read_csv('Data/tweets_to_predict.csv')\n",
    "answers = pd.read_csv('Data/annotated_tweets.csv', delimiter=';', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Text preprocessing\n",
    "\n",
    "* Select features\n",
    "* Spell correction\n",
    "* Normalize text\n",
    "* Extract domains from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = ['status_id', 'created_at', 'text', 'favorite_count', 'retweet_count']\n",
    "tweets = tweets[to_keep]\n",
    "tweets = tweets.merge(answers, on='status_id', how='inner')\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = pd.read_excel(\"Data/keywords.xlsx\")\n",
    "\n",
    "politics = list(keywords[keywords.Politics.isna()==False].Politics)\n",
    "travel = list(keywords[keywords.Travel.isna()==False].Travel)\n",
    "personal = list(keywords[keywords['Personal impact'].isna()==False]['Personal impact'])\n",
    "health = list(keywords[keywords.Health.isna()==False].Health)\n",
    "economic = list(keywords[keywords['Economic impact'].isna()==False]['Economic impact'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', \n",
    "        'time', 'date', 'number'],\n",
    "    \n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=True,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")\n",
    "\n",
    "tweets['corrected_text'] = [\" \".join(text_processor.pre_process_doc(s)) for s in tweets.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopwords = nltk.corpus.stopwords.words('english')\n",
    "word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem #stemmer\n",
    "lem = nltk.stem.WordNetLemmatizer().lemmatize #lemmatizer\n",
    "my_punctuation = '!\"$%&\\'()*+,-./:;=?[\\\\]^_`{|}~â€¢'\n",
    "\n",
    "\n",
    "def get_word_and_tag(tokens):\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    cleaned_tags = []\n",
    "    for word, tag in tagged:\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        cleaned_tags.append((word,pos))\n",
    "    return cleaned_tags\n",
    "\n",
    "def clean_tweet_word2vec(tweet, bigrams=False):\n",
    "    tweet = tweet.lower() # lower case\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    tweet = re.sub('['+my_punctuation + ']+', ' ', tweet) # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet) #remove double spacing\n",
    "    tweet = re.sub('([0-9]+)', '', tweet) # remove numbers\n",
    "    tweet_token_list = [word for word in tweet.split(' ') if word not in my_stopwords] # remove stopwords\n",
    "    tweet_token_list = [word for word in tweet_token_list if len(word)>0]\n",
    "    \n",
    "    tweet_token_list = [lem(word,tag) for word,tag in get_word_and_tag(tweet_token_list)] # apply word rooter\n",
    "    if bigrams:\n",
    "        tweet_token_list = tweet_token_list+[tweet_token_list[i]+'_'+tweet_token_list[i+1]\n",
    "                                            for i in range(len(tweet_token_list)-1)]\n",
    "    tweet = ' '.join(tweet_token_list)\n",
    "    return tweet\n",
    "\n",
    "tweets['corrected_text_w2v'] = tweets['corrected_text'].apply(clean_tweet_word2vec, bigrams=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP features\n",
    "tweets['char_count'] = tweets['corrected_text'].apply(len)\n",
    "tweets['word_count'] = tweets['corrected_text'].apply(lambda x: len(x.split()))\n",
    "tweets['word_density'] = tweets['char_count'] / (tweets['word_count']+1)\n",
    "tweets['punctuation_count'] = tweets['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "tweets['title_word_count'] = tweets['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "tweets['upper_case_word_count'] = tweets['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_cols = ['Politics_kw', 'Health_kw', 'Personal_kw', 'Economic_kw', 'Travel_kw'] + ['Politics_perc', 'Health_perc', 'Personal_perc', 'Economic_perc', 'Travel_perc']\n",
    "\n",
    "for i, row in tweets.iterrows():\n",
    "    text = row['text'].lower()\n",
    "    for l, dim, perc in zip([politics, health, personal, economic, travel],['Politics_kw', 'Health_kw', 'Personal_kw', 'Economic_kw', 'Travel_kw'], ['Politics_perc', 'Health_perc', 'Personal_perc', 'Economic_perc', 'Travel_perc']):\n",
    "        k = 0\n",
    "        for j in l:\n",
    "            k += text.count(j.lower())\n",
    "        tweets.loc[i,dim] = k\n",
    "        tweets.loc[i,perc] = k/row['word_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read not annotated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_to_train['corrected_text'] = [\" \".join(text_processor.pre_process_doc(s)) for s in tweets_to_train.text]\n",
    "tweets_to_train['corrected_text_w2v'] = tweets_to_train['corrected_text'].apply(clean_tweet_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP features\n",
    "tweets_to_train['char_count'] = tweets_to_train['corrected_text'].apply(len)\n",
    "tweets_to_train['word_count'] = tweets_to_train['corrected_text'].apply(lambda x: len(x.split()))\n",
    "tweets_to_train['word_density'] = tweets_to_train['char_count'] / (tweets_to_train['word_count']+1)\n",
    "tweets_to_train['punctuation_count'] = tweets_to_train['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "tweets_to_train['title_word_count'] = tweets_to_train['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "tweets_to_train['upper_case_word_count'] = tweets_to_train['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in tweets_to_train.iterrows():\n",
    "    text = row['text'].lower()\n",
    "    for l, dim, perc in zip([politics, health, personal, economic, travel],['Politics_kw', 'Health_kw', 'Personal_kw', 'Economic_kw', 'Travel_kw'], ['Politics_perc', 'Health_perc', 'Personal_perc', 'Economic_perc', 'Travel_perc']):\n",
    "        k = 0\n",
    "        for j in l:\n",
    "            k += text.count(j.lower())\n",
    "        tweets_to_train.loc[i,dim] = k\n",
    "        tweets_to_train.loc[i,perc] = k/row['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "cols = ['Politics_kw', 'Health_kw', 'Personal_kw', 'Economic_kw', 'Travel_kw', 'char_count', \n",
    "        'word_count', 'word_density','punctuation_count', 'title_word_count', 'upper_case_word_count', \n",
    "        'Politics_perc','Health_perc', 'Personal_perc', 'Economic_perc', 'Travel_perc', 'text', 'corrected_text', \n",
    "       'corrected_text_w2v']\n",
    "categories = ['Politics', 'Health', 'Economic', 'Personal', 'Travel'] \n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(tweets[cols], tweets[categories], \n",
    "                                                                      test_size = 0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "* Count Vectors as features\n",
    "* TF-IDF Vectors as features\n",
    "    * Word level\n",
    "    * N-Gram level\n",
    "    * Character level (useless)\n",
    "* Word Embeddings as features\n",
    "* Text / NLP based features\n",
    "* Dimensionality reduction topic models\n",
    "* Word embeddings\n",
    "    * Word2Vec\n",
    "    * Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "count_vect.fit(tweets_to_train.corrected_text_w2v)\n",
    "\n",
    "x_count = count_vect.transform(tweets_to_train.corrected_text_w2v)\n",
    "xtrain_count =  count_vect.transform(train_x.corrected_text_w2v)\n",
    "xvalid_count =  count_vect.transform(valid_x.corrected_text_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(tweets_to_train.corrected_text)\n",
    "\n",
    "x_tfidf = tfidf_vect.transform(tweets_to_train.corrected_text)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x.corrected_text)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x.corrected_text)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(tweets_to_train.corrected_text)\n",
    "\n",
    "x_tfidf_ngram = tfidf_vect_ngram.transform(tweets_to_train.corrected_text)\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x.corrected_text)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x.corrected_text)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(tweets_to_train.corrected_text)\n",
    "\n",
    "x_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(tweets_to_train.corrected_text)\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x.corrected_text) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x.corrected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = [word_tokenize(i) for i in train_x.corrected_text_w2v]\n",
    "valid_tokenized = [word_tokenize(i) for i in valid_x.corrected_text_w2v]\n",
    "\n",
    "all_text = [word_tokenize(i) for i in tweets_to_train.corrected_text_w2v]\n",
    "\n",
    "all_text += train_tokenized+valid_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=10,window=5, sample=6e-5, alpha=0.03,  min_alpha=0.0007, negative=20)\n",
    "\n",
    "w2v_model.build_vocab(all_text)\n",
    "\n",
    "w2v_model.train(all_text, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.wv.vocab:\n",
    "            mean.append(wv.wv.syn0[wv.wv.vocab[word].index])\n",
    "            all_words.add(wv.wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        #logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])\n",
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenized = valid_x.apply(lambda r: w2v_tokenize_text(r['corrected_text_w2v']), axis=1).values\n",
    "train_tokenized = train_x.apply(lambda r: w2v_tokenize_text(r['corrected_text_w2v']), axis=1).values\n",
    "new_tokenized = tweets_to_train.apply(lambda r: w2v_tokenize_text(r['corrected_text_w2v']), axis=1).values\n",
    "\n",
    "X_train_word_average = word_averaging_list(w2v_model,train_tokenized)\n",
    "X_valid_word_average = word_averaging_list(w2v_model,test_tokenized)\n",
    "new_word_average = word_averaging_list(w2v_model, new_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def label_sentences(corpus, label_type):\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v.split(), [label]))\n",
    "    return labeled\n",
    "\n",
    "train_sentences = label_sentences(train_x.corrected_text_w2v, 'Train')\n",
    "valid_sentences = label_sentences(valid_x.corrected_text_w2v, 'Test')\n",
    "new_sentences = label_sentences(tweets_to_train.corrected_text_w2v, 'New')\n",
    "all_data = new_sentences + train_sentences + valid_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors\n",
    "\n",
    "def tune_doc2vec():\n",
    "    best_f1 = 0\n",
    "    for vector_size in [100,200,300,400,500,600]:\n",
    "        for negative in [5,10]:\n",
    "            model_dbow = Doc2Vec(dm=0, vector_size=vector_size, negative=negative, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "            model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
    "\n",
    "            for epoch in range(30):\n",
    "                model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "                model_dbow.alpha -= 0.002\n",
    "                model_dbow.min_alpha = model_dbow.alpha\n",
    "\n",
    "            train_temp = get_vectors(model_dbow, len(train_sentences), vector_size, 'Train')\n",
    "            test_temp = get_vectors(model_dbow, len(valid_sentences), vector_size, 'Test')\n",
    "\n",
    "            accuracy = train_model(linear_model.LogisticRegression(), train_temp, train_y[col], test_temp, probs=False)\n",
    "            if accuracy[0]>best_f1:\n",
    "                best_f1 = accuracy[0]\n",
    "                train_vectors_dbow = train_temp\n",
    "                test_vectors_dbow = test_temp\n",
    "\n",
    "    return train_vectors_dbow, test_vectors_dbow\n",
    "\n",
    "def get_doc2vec(vector_size, negative):\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=vector_size, negative=negative, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "    model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
    "\n",
    "    for epoch in range(30):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "\n",
    "    train_temp = get_vectors(model_dbow, len(train_sentences), vector_size, 'Train')\n",
    "    test_temp = get_vectors(model_dbow, len(valid_sentences), vector_size, 'Test')\n",
    "    new_temp = get_vectors(model_dbow, len(new_sentences), vector_size, 'New')\n",
    "\n",
    "    return train_temp, test_temp, new_temp\n",
    "\n",
    "#train_vectors_dbow, test_vectors_dbow = tune_doc2vec()\n",
    "train_vectors_dbow, test_vectors_dbow, new_vectors_dbow = get_doc2vec(300,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a LDA Model\n",
    "def get_lda_topics(tweets, n, features = 'count', purpose = 'test'):\n",
    "    x_train_count =  xtrain_count\n",
    "    x_valid_count =  xvalid_count\n",
    "    count = x_count\n",
    "    if features == 'tfidf':\n",
    "        x_train_count =  xtrain_tfidf\n",
    "        x_valid_count =  xvalid_tfidf\n",
    "        count = x_tfidf\n",
    "    elif features == 'ngram':\n",
    "        x_train_count =  xtrain_tfidf_ngram\n",
    "        x_valid_count =  xvalid_tfidf_ngram\n",
    "        count = x_tfidf_ngram\n",
    "    elif features == 'char':\n",
    "        x_train_count =  xtrain_tfidf_ngram_chars\n",
    "        x_valid_count =  xvalid_tfidf_ngram_chars\n",
    "        count = x_tfidf_ngram_chars\n",
    "\n",
    "    lda_model = decomposition.LatentDirichletAllocation(n_components=n, learning_method='online', max_iter=20)\n",
    "    lda_model.fit(count)\n",
    "\n",
    "    train_topics = lda_model.transform(x_train_count)\n",
    "    valid_topics = lda_model.transform(x_valid_count)\n",
    "    new_topics = lda_model.transform(count)\n",
    "    \n",
    "    if purpose == 'test':\n",
    "        accuracy = train_model(linear_model.SGDClassifier(alpha=0.00001), train_topics, train_y[col], valid_topics, probs=False)\n",
    "        return accuracy[0]\n",
    "    \n",
    "    else:\n",
    "        return train_topics, valid_topics, new_topics\n",
    "\n",
    "def tune_lda():\n",
    "    best_features = ''\n",
    "    best_n = 0\n",
    "    best_f1 = 0\n",
    "    for features in ['count', 'tfidf', 'ngram', 'char']:\n",
    "        for n in [15,30,45,60]:\n",
    "            f1 = get_lda_topics(tweets, n, features = features, purpose = 'test')\n",
    "            if f1 > best_f1:\n",
    "                best_n = n\n",
    "                best_features = features\n",
    "                best_f1 = f1\n",
    "    return best_n, best_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lr(train, test):\n",
    "    best_f1 = 0\n",
    "    best_c = 1\n",
    "    best_weight = 1\n",
    "    for w in [0.3,0.5,0.7,0.9]:\n",
    "        for c in np.linspace(0.01,250,100):\n",
    "            lr = linear_model.LogisticRegression(C=c, random_state=0, solver = 'lbfgs', max_iter=1000, class_weight={0:w,1:1})\n",
    "            f1 = train_model(lr, train, train_y[col], test, probs=False)[0]\n",
    "            if f1>best_f1:\n",
    "                best_f1 = f1\n",
    "                best_c = c\n",
    "                best_weight = w\n",
    "    print('Logistic Regression: {}, {}'.format(best_c, best_f1))\n",
    "    return linear_model.LogisticRegression(C = best_c, random_state=0, class_weight={0:best_weight,1:1}), best_f1\n",
    "\n",
    "def tune_rc(train, test):\n",
    "    best_f1 = 0\n",
    "    best_alpha = 1\n",
    "    best_weight = 1\n",
    "    for w in [0.3,0.5,0.7,0.9]:\n",
    "        for alpha in np.linspace(0.001,50,100):\n",
    "            lr = linear_model.RidgeClassifier(alpha = alpha, random_state=0, class_weight={0:w,1:1})\n",
    "            f1 = train_model(lr, train, train_y[col], test, probs=False)[0]\n",
    "            if f1>best_f1:\n",
    "                best_f1 = f1\n",
    "                best_alpha = alpha\n",
    "                best_weight = w\n",
    "    print('Ridge Classifier: {}, {}'.format(best_alpha, best_f1))\n",
    "    return linear_model.RidgeClassifier(alpha=best_alpha, random_state=0, class_weight={0:best_weight,1:1}), best_f1\n",
    "\n",
    "def tune_sgd(train, test):\n",
    "    best_f1 = 0\n",
    "    best_alpha = 0.0001\n",
    "    best_alpha = 1\n",
    "    best_weight = 1\n",
    "    for w in [0.3,0.5,0.7,0.9]:\n",
    "        for alpha in np.linspace(0.0000001,0.01,100):\n",
    "            lr = linear_model.SGDClassifier(alpha = alpha, random_state=0, class_weight={0:w,1:1})\n",
    "            f1 = train_model(lr, train, train_y[col], test, probs=False)[0]\n",
    "            if f1>best_f1:\n",
    "                best_f1 = f1\n",
    "                best_alpha = alpha\n",
    "                best_weight = w\n",
    "    print('SGD Classifier: {}, {}'.format(best_alpha, best_f1))\n",
    "    return linear_model.SGDClassifier(alpha = best_alpha, random_state=0, class_weight={0:best_weight,1:1}), best_f1\n",
    "\n",
    "def tune_svc(train, test):\n",
    "    best_f1 = 0\n",
    "    best_c = 1\n",
    "    best_alpha = 1\n",
    "    best_weight = 1\n",
    "    for w in [0.3,0.5,0.7,0.9]:\n",
    "        for c in np.linspace(0.5,30,40):\n",
    "            # lr = svm.SVC(C=c, random_state=0, kernel = 'linear', gamma='auto', class_weight={0:0.9,1:1})\n",
    "            lr = svm.LinearSVC(C=c, random_state=0, class_weight={0:w,1:1})\n",
    "            f1 = train_model(lr, train, train_y[col], test, probs=False)[0]\n",
    "            if f1>best_f1:\n",
    "                best_f1 = f1\n",
    "                best_c = c\n",
    "                best_weight = w\n",
    "    print('SVC: {}, {}'.format(best_c, best_f1))\n",
    "    return svm.LinearSVC(C = best_c, random_state=0, class_weight={0:best_weight,1:1}), best_f1\n",
    "\n",
    "#tune_svc(X_train_word_average, X_valid_word_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model_selector(train, test):\n",
    "    lr, lr_f1 = tune_lr(train, test)\n",
    "    rc, rc_f1 = tune_rc(train, test)\n",
    "    sgd, sgd_f1 = tune_sgd(train, test)\n",
    "    svc, svc_f1 = tune_svc(train, test)\n",
    "    max_f1 = max([lr_f1,rc_f1,sgd_f1,svc_f1])\n",
    "    if max_f1 == lr_f1:\n",
    "        print(lr)\n",
    "        return lr\n",
    "    elif max_f1 == rc_f1:\n",
    "        print(rc)\n",
    "        return rc\n",
    "    elif max_f1 == sgd_f1:\n",
    "        print(sgd)\n",
    "        return sgd\n",
    "    else:\n",
    "        print(svc)\n",
    "        return svc\n",
    "    \n",
    "def model_selector(train, test):\n",
    "    lr, lr_f1 = tune_lr(train, test)\n",
    "    rc, rc_f1 = tune_rc(train, test)\n",
    "    sgd, sgd_f1 = tune_sgd(train, test)\n",
    "    max_f1 = max([lr_f1,rc_f1,sgd_f1])\n",
    "    if max_f1 == lr_f1:\n",
    "        print(lr)\n",
    "        return lr\n",
    "    elif max_f1 == rc_f1:\n",
    "        print(rc)\n",
    "        return rc\n",
    "    else:\n",
    "        print(sgd)\n",
    "        return sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def set_builder():\n",
    "    sets = [(X_train_word_average, X_valid_word_average, 'word2vec', new_word_average),\n",
    "           (train_topics, valid_topics, 'svd_components', new_topics),\n",
    "           (xtrain_tfidf, xvalid_tfidf, 'word_tfidf', x_count),\n",
    "           (xtrain_count, xvalid_count, 'word_count', x_tfidf),\n",
    "           (train_x[cols[:-3]], valid_x[cols[:-3]], 'nlp_features', tweets_to_train[cols[:-3]]),\n",
    "           (train_vectors_dbow, test_vectors_dbow, 'doc2vec', new_vectors_dbow)]\n",
    "    \n",
    "    valid = pd.DataFrame(index = valid_x.index.values, columns = [i[2] for i in sets])\n",
    "    new = pd.DataFrame(index = tweets_to_train.index.values, columns = [i[2] for i in sets])\n",
    "    \n",
    "    for s in sets:\n",
    "        classifier = model_selector(s[0],s[1])\n",
    "        if type(classifier).__name__ == 'LogisticRegression':\n",
    "            new[s[2]] =  train_model(classifier, s[0], train_y[col], s[3], probs=True, ret_probas=True, valid = False)\n",
    "            valid[s[2]] =  train_model(classifier, s[0], train_y[col], s[1], probs=True, ret_probas=True)\n",
    "        else:\n",
    "            new[s[2]] =  train_model(classifier, s[0], train_y[col], s[3], probs=False, ret_preds=True, valid = False)\n",
    "            valid[s[2]] =  train_model(classifier, s[0], train_y[col], s[1], probs=False, ret_preds=True)\n",
    "            \n",
    "    return new, valid\n",
    "\n",
    "#%time train_preds, valid_preds = set_builder('Politics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models and make predictions for unseen tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Politics'\n",
    "n, features = tune_lda()\n",
    "train_topics, valid_topics, new_topics = get_lda_topics(tweets, n, features=features, purpose = 'gimme')\n",
    "print(n,features)\n",
    "%time new_preds_pol, valid_preds_pol= set_builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Health'\n",
    "n, features = tune_lda()\n",
    "train_topics, valid_topics, new_topics = get_lda_topics(tweets, n, features=features, purpose = 'gimme')\n",
    "print(n,features)\n",
    "%time new_preds_hl, valid_preds_hl = set_builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Economic'\n",
    "n, features = tune_lda()\n",
    "train_topics, valid_topics, new_topics = get_lda_topics(tweets, n, features=features, purpose = 'gimme')\n",
    "print(n,features)\n",
    "%time new_preds_ec, valid_preds_ec = set_builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Travel'\n",
    "n, features = tune_lda()\n",
    "train_topics, valid_topics, new_topics = get_lda_topics(tweets, n, features=features, purpose = 'gimme')\n",
    "print(n,features)\n",
    "%time new_preds_tr, valid_preds_tr = set_builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Personal'\n",
    "n, features = tune_lda()\n",
    "train_topics, valid_topics, new_topics = get_lda_topics(tweets, n, features=features, purpose = 'gimme')\n",
    "print(n,features)\n",
    "%time new_preds_per, valid_preds_per = set_builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_preds_ec['y'] = list(valid_y['Economic'])\n",
    "valid_preds_per['y'] = list(valid_y['Personal'])\n",
    "valid_preds_pol['y'] = list(valid_y['Politics'])\n",
    "valid_preds_tr['y'] = list(valid_y['Travel'])\n",
    "valid_preds_hl['y'] = list(valid_y['Health'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate predictions using a meta-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = ensemble.BaggingClassifier()\n",
    "\n",
    "#Politics\n",
    "meta.fit(valid_preds_pol.drop('y', axis = 1), valid_preds_pol['y'])\n",
    "tweets_to_train['Politics'] = meta.predict(new_preds_pol)\n",
    "\n",
    "#Travel\n",
    "meta.fit(valid_preds_tr.drop('y', axis = 1), valid_preds_tr['y'])\n",
    "tweets_to_train['Travel'] = meta.predict(new_preds_tr)\n",
    "\n",
    "#Economic\n",
    "meta.fit(valid_preds_ec.drop('y', axis = 1), valid_preds_ec['y'])\n",
    "tweets_to_train['Economic'] = meta.predict(new_preds_ec)\n",
    "\n",
    "#Health\n",
    "meta.fit(valid_preds_he.drop('y', axis = 1), valid_preds_he['y'])\n",
    "tweets_to_train['Health'] = meta.predict(new_preds_he)\n",
    "\n",
    "#Personal\n",
    "meta.fit(valid_preds_per.drop('y', axis = 1), valid_preds_per['y'])\n",
    "tweets_to_train['Personal'] = meta.predict(new_preds_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_to_train.to_csv('Data/tweets_with_categories.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning\n",
    "\n",
    "* Select a subset of 100 tweets from each category to be given for human annotation\n",
    "* The process was repeated 4 times\n",
    "* The file answers2.csv contains all the annotated tweets including the ones given for the initial annotation and the ones given during the active learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['word2vec', 'svd_components', 'word_tfidf', 'word_count', 'nlp_features', 'doc2vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in predictors:\n",
    "    new_preds_ec[c] = [1 if i>=0.5 else 0 for i in new_preds_ec[c]]\n",
    "    new_preds_hl[c] = [1 if i>=0.5 else 0 for i in new_preds_hl[c]]\n",
    "    new_preds_tr[c] = [1 if i>=0.5 else 0 for i in new_preds_tr[c]]\n",
    "    new_preds_pol[c] = [1 if i>=0.5 else 0 for i in new_preds_pol[c]]\n",
    "    new_preds_per[c] = [1 if i>=0.5 else 0 for i in new_preds_per[c]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_preds_ec['pred'] = new_preds_ec[predictors].sum(axis=1)\n",
    "new_preds_pol['pred'] = new_preds_pol[predictors].sum(axis=1)\n",
    "new_preds_hl['pred'] = new_preds_hl[predictors].sum(axis=1)\n",
    "new_preds_per['pred'] = new_preds_per[predictors].sum(axis=1)\n",
    "new_preds_tr['pred'] = new_preds_tr[predictors].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_to_train['Politics'] = new_preds_pol['pred']\n",
    "tweets_to_train['Health'] = new_preds_hl['pred']\n",
    "tweets_to_train['Personal'] = new_preds_per['pred']\n",
    "tweets_to_train['Travel'] = new_preds_tr['pred']\n",
    "tweets_to_train['Economic'] = new_preds_ec['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_to_train.dropna().to_csv('wtf.csv')\n",
    "tweets_to_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al_list = []\n",
    "\n",
    "for i in categories:\n",
    "    print(i)\n",
    "    al_list += list(tweets_to_train[(tweets_to_train[i] == 3)].sample(100)['status_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(al_list).to_csv('Data/tweets_for_active_learning.csv', index=False, header= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
